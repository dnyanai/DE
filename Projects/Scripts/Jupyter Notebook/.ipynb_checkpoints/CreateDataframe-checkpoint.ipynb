{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e4ba49-2c70-4051-8269-e92195523905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##!conda install pandas -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606ad91-6035-4ef1-9f8f-2a30317e0dbb",
   "metadata": {},
   "source": [
    "# Find dataset in the provided data folder and Loading Datasets into a dataframes dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7ef6bc-534f-4745-8413-99b2390d58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfc72d9-5aa7-4873-8265-dd045a5b0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302aa478-81df-4e75-876e-9f4c958bfb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_make_dataset(folder_name, dataset):\n",
    "    dataset_path = \"\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"/Users/my-air/Desktop/DSA/Github/Data Engineering/DE-Notes/Projects\"):\n",
    "            try:    \n",
    "                if folder_name in dirs:\n",
    "                    data_file_path = os.path.join(root,folder_name)\n",
    "                    print(f\"Dataset folder found at: {data_file_path} \\n\")\n",
    "                    \n",
    "                    for d_root, d_dirs, d_files in os.walk(data_file_path):\n",
    "                        try:\n",
    "                            visible_files = [each for each in d_files if not each.startswith('.')] #ignore hidden files and empty\n",
    "                        \n",
    "                            for file in visible_files:\n",
    "                                    if dataset in file:\n",
    "                                            dataset_path = os.path.join(data_file_path,file)\n",
    "                                            print(f\"Dataset file found: {dataset_path.split('/')[-1]}\\n\")\n",
    "                                            convert_zip_to_dataframe(dataset, dataset_path)\n",
    "                                            print(f\"Dataset converted in a DataFrame and added to Dataframes,{dataframes.keys()}\")\n",
    "                        except Exception as e:\n",
    "                                print(f\"{e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{e}\")  \n",
    "    return dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b589be73-af26-468a-9eee-069378d10afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_zip_to_dataframe(dataset_name, dataset_path):    \n",
    "    \n",
    "#     if not os.path.isfile(dataset_path):\n",
    "#             raise FileNotFoundError(\"File not found: {dataset_path}\")\n",
    "#     else:\n",
    "#         print(f\"Processing file: {dataset_path} \\n\") #process stage update\n",
    "#         with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "#             dataframes[dataset_name] = {}\n",
    "            \n",
    "#             file_list = zip_ref.namelist()\n",
    "            \n",
    "#             for file in file_list:\n",
    "#                 if '.csv' in file:\n",
    "#                     try:\n",
    "#                         print(f\"Extracting {file} and converting it to a dataframe \\n\") #process stage update\n",
    "#                         extracted_file = zip_ref.extract(file)\n",
    "                      \n",
    "#                         df = pd.read_csv(extracted_file) #extract the file and read csv into a pandas dataframe\n",
    "\n",
    "#                         dataframes[dataset_name][file.split('/')[1]] = df\n",
    "                            \n",
    "#                         print(f\"Sucessfully processed the data file: {file} \\n\") \n",
    "\n",
    "#                         #Cleanup\n",
    "#                         os.remove(file)\n",
    "#                         print(f\"{file} removed \\n\")\n",
    "#                         os.rmdir(file.split('/')[0])\n",
    "#                         print(f\"Directory removed\") \n",
    "                        \n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error reading the csv file {file}: {e}\")\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3daffa-64c5-4457-bd1b-11d152b57f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_zip_to_dataframe(dataset_name, dataset_path):\n",
    "    \"\"\"\n",
    "   Convert zip file to pandas dataframes\n",
    "\n",
    "   Args:\n",
    "   dataset_name (str): Name of the dataset file to find\n",
    "   dataset_path (str): Path to the zip file\n",
    "   Returns:\n",
    "\n",
    "   \"\"\"\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        raise FileNotFoundError(\"File not found: {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"Processing file: {dataset_path} \\n\")  #process stage update\n",
    "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "            \"\"\"\n",
    "            This is not needed right now as we are not storing dataframes in any place\n",
    "            \"\"\"\n",
    "            # if dataset_name in dataframes.keys():\n",
    "            #     print(\"dataset converted to dataframe already please check dataframes.keys()\")\n",
    "            # else:\n",
    "            dataframes[dataset_name] = {}\n",
    "\n",
    "            file_list = zip_ref.namelist()\n",
    "            print(file_list)\n",
    "\n",
    "            for file in [file for file in file_list if not file.endswith('/')]:\n",
    "                \n",
    "                if '.csv' in file:\n",
    "                    try:\n",
    "                        print(f\"Extracting {file} and converting it to a dataframe \\n\")  #process stage update\n",
    "                        extracted_file = zip_ref.extract(file)\n",
    "\n",
    "                        df = pd.read_csv(extracted_file)  #extract the file and read csv into a pandas dataframe\n",
    "\n",
    "                        dataframes[dataset_name][file.split('/')[1]] = df\n",
    "\n",
    "                        print(f\"Sucessfully processed the data file: {file} \\n\")\n",
    "\n",
    "                        #Cleanup\n",
    "                        os.remove(file)\n",
    "                        print(f\"{file} removed \\n\")\n",
    "                        os.rmdir(file.split('/')[0])\n",
    "                        print(f\"Directory removed\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading the csv file {file}: {e}\")\n",
    "\n",
    "                elif '.xl' in file:\n",
    "                    try:\n",
    "                        print(f\"{file} in zipped folder\")\n",
    "\n",
    "                        print(f\"Extracting {file} and converting it to a dataframe \\n\")  #process stage update\n",
    "                        extracted_file = zip_ref.extract(file)\n",
    "                        \n",
    "                        print(extracted_file)\n",
    "                        \n",
    "                        df = pd.read_excel(extracted_file, engine=\"openpyxl\")  #extract the file and read csv into a pandas dataframe\n",
    "                        print(1)\n",
    "                        dataframes[dataset_name][file.split('/')[1]] = df\n",
    "\n",
    "                        print(f\"Sucessfully processed the data file: {file} \\n\")\n",
    "\n",
    "                        #Cleanup\n",
    "                        os.remove(file)\n",
    "                        print(f\"{file} removed \\n\")\n",
    "                        os.rmdir(file.split('/')[0])\n",
    "                        print(f\"Directory removed\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading the excel file {file}: {e}\")\n",
    "                \n",
    "                else: \n",
    "                    print(f\"add new extension format to code for CreateDataset.py: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b408f4c-4110-4202-a805-9ac72e855638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset folder found at: /Users/my-air/Desktop/DSA/Github/Data Engineering/DE-Notes/Projects/data \n",
      "\n",
      "Dataset file found: aerofit.zip\n",
      "\n",
      "Processing file: /Users/my-air/Desktop/DSA/Github/Data Engineering/DE-Notes/Projects/data/aerofit.zip \n",
      "\n",
      "['datasets/', 'datasets/aerofit_treadmill_data.csv']\n",
      "Extracting datasets/aerofit_treadmill_data.csv and converting it to a dataframe \n",
      "\n",
      "Sucessfully processed the data file: datasets/aerofit_treadmill_data.csv \n",
      "\n",
      "datasets/aerofit_treadmill_data.csv removed \n",
      "\n",
      "Directory removed\n",
      "Dataset converted in a DataFrame and added to Dataframes,dict_keys(['aerofit'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aerofit': {'aerofit_treadmill_data.csv':     Product  Age  Gender  Education MaritalStatus  Usage  Fitness  Income  \\\n",
       "  0     KP281   18    Male         14        Single      3        4   29562   \n",
       "  1     KP281   19    Male         15        Single      2        3   31836   \n",
       "  2     KP281   19  Female         14     Partnered      4        3   30699   \n",
       "  3     KP281   19    Male         12        Single      3        3   32973   \n",
       "  4     KP281   20    Male         13     Partnered      4        2   35247   \n",
       "  ..      ...  ...     ...        ...           ...    ...      ...     ...   \n",
       "  175   KP781   40    Male         21        Single      6        5   83416   \n",
       "  176   KP781   42    Male         18        Single      5        4   89641   \n",
       "  177   KP781   45    Male         16        Single      5        5   90886   \n",
       "  178   KP781   47    Male         18     Partnered      4        5  104581   \n",
       "  179   KP781   48    Male         18     Partnered      4        5   95508   \n",
       "  \n",
       "       Miles  \n",
       "  0      112  \n",
       "  1       75  \n",
       "  2       66  \n",
       "  3       85  \n",
       "  4       47  \n",
       "  ..     ...  \n",
       "  175    200  \n",
       "  176    200  \n",
       "  177    160  \n",
       "  178    120  \n",
       "  179    180  \n",
       "  \n",
       "  [180 rows x 9 columns]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_and_make_dataset(\"data\", \"aerofit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54959f88-6cfb-4689-9707-46da5419d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aerofit'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c809d8-319b-45ea-b35d-db4bbd8b049d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aerofit_treadmill_data.csv':     Product  Age  Gender  Education MaritalStatus  Usage  Fitness  Income  \\\n",
       " 0     KP281   18    Male         14        Single      3        4   29562   \n",
       " 1     KP281   19    Male         15        Single      2        3   31836   \n",
       " 2     KP281   19  Female         14     Partnered      4        3   30699   \n",
       " 3     KP281   19    Male         12        Single      3        3   32973   \n",
       " 4     KP281   20    Male         13     Partnered      4        2   35247   \n",
       " ..      ...  ...     ...        ...           ...    ...      ...     ...   \n",
       " 175   KP781   40    Male         21        Single      6        5   83416   \n",
       " 176   KP781   42    Male         18        Single      5        4   89641   \n",
       " 177   KP781   45    Male         16        Single      5        5   90886   \n",
       " 178   KP781   47    Male         18     Partnered      4        5  104581   \n",
       " 179   KP781   48    Male         18     Partnered      4        5   95508   \n",
       " \n",
       "      Miles  \n",
       " 0      112  \n",
       " 1       75  \n",
       " 2       66  \n",
       " 3       85  \n",
       " 4       47  \n",
       " ..     ...  \n",
       " 175    200  \n",
       " 176    200  \n",
       " 177    160  \n",
       " 178    120  \n",
       " 179    180  \n",
       " \n",
       " [180 rows x 9 columns]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[\"aerofit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3e235-c22b-41a2-93d6-c53eb3d46ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
